"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["466793"],{76253:function(e,n,i){i.r(n),i.d(n,{default:()=>h,frontMatter:()=>d,metadata:()=>t,assets:()=>c,toc:()=>l,contentTitle:()=>a});var t=JSON.parse('{"id":"ai/vector-search","title":"Vector Search","description":"\x3c!--","source":"@site/docs/ai/vector-search.md","sourceDirName":"ai","slug":"/ai/vector-search","permalink":"/docs/dev/ai/vector-search","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Vector Search","language":"en"},"sidebar":"docs","previous":{"title":"AI Function","permalink":"/docs/dev/ai/ai-function-overview"},"next":{"title":"Lakehouse Overview","permalink":"/docs/dev/lakehouse/lakehouse-overview"}}'),r=i("785893"),s=i("250065");let d={title:"Vector Search",language:"en"},a=void 0,c={},l=[{value:"Brute-Force Search",id:"brute-force-search",level:2},{value:"Approximate Nearest Neighbor Search",id:"approximate-nearest-neighbor-search",level:2},{value:"Approximate Range Search",id:"approximate-range-search",level:2},{value:"Compound Search",id:"compound-search",level:2},{value:"ANN Search with Additional Filters",id:"ann-search-with-additional-filters",level:2},{value:"Session Variables Related to ANN Search",id:"session-variables-related-to-ann-search",level:2},{value:"Vector Quantization",id:"vector-quantization",level:2},{value:"Performance Tuning",id:"performance-tuning",level:2},{value:"Query Performance",id:"query-performance",level:3},{value:"Use Prepared Statements",id:"use-prepared-statements",level:3},{value:"Reduce Segment Count",id:"reduce-segment-count",level:3},{value:"Reduce Rowset Count",id:"reduce-rowset-count",level:3},{value:"Keep ANN Index in Memory",id:"keep-ann-index-in-memory",level:3},{value:"parallel_pipeline_task_num = 1",id:"parallel_pipeline_task_num--1",level:3},{value:"enable_profile = false",id:"enable_profile--false",level:3},{value:"Usage Limitations",id:"usage-limitations",level:2}];function o(e){let n={a:"a",br:"br",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"In generative AI applications, relying solely on a large model's internal parameter \u201Cmemory\u201D has clear limitations: (1) the model\u2019s knowledge becomes outdated and cannot cover the latest information; (2) directly asking the model to \u201Cgenerate\u201D answers increases the risk of hallucinations. This gives rise to RAG (Retrieval-Augmented Generation). The key task of RAG is not to have the model fabricate answers from nothing, but to retrieve the Top-K most relevant information chunks from an external knowledge base and feed them to the model as grounding context."}),"\n",(0,r.jsx)(n.p,{children:"To achieve this, we need a mechanism to measure semantic relatedness between a user query and documents in the knowledge base. Vector representations are a standard tool: by encoding both queries and documents into semantic vectors, we can use vector similarity to measure relevance. With the advancement of pretrained language models, generating high-quality embeddings has become mainstream. Thus, the retrieval stage of RAG becomes a typical vector similarity search problem: from a large vector collection, find the K vectors most similar to the query (i.e., candidate knowledge pieces)."}),"\n",(0,r.jsx)(n.p,{children:"Vector retrieval in RAG is not limited to text; it naturally extends to multimodal scenarios. In a multimodal RAG system, images, audio, video, and other data types can also be encoded into vectors for retrieval and then supplied to the generative model as context. For example, if a user uploads an image, the system can first retrieve related descriptions or knowledge snippets, then generate explanatory content. In medical QA, RAG can retrieve patient records and literature to support more accurate diagnostic suggestions."}),"\n",(0,r.jsx)(n.h2,{id:"brute-force-search",children:"Brute-Force Search"}),"\n",(0,r.jsx)(n.p,{children:"Starting from version 2.0, Apache Doris supports nearest-neighbor search based on vector distance. Performing vector search with SQL is natural and simple:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"SELECT id,\n       l2_distance(embedding, [1.0, 2.0, xxx, 10.0]) AS distance\nFROM   vector_table\nORDER  BY distance\nLIMIT  10; \n"})}),"\n",(0,r.jsx)(n.p,{children:"When the dataset is small (under ~1 million rows), Doris\u2019s exact K-Nearest Neighbor search performance is sufficient, providing 100% recall and precision. As the dataset grows, however, most users are willing to trade a small amount of recall/accuracy for significantly lower latency. The problem then becomes Approximate Nearest Neighbor (ANN) search."}),"\n",(0,r.jsx)(n.h2,{id:"approximate-nearest-neighbor-search",children:"Approximate Nearest Neighbor Search"}),"\n",(0,r.jsx)(n.p,{children:"From version 4.0, Apache Doris officially supports ANN search. No additional data type is introduced: vectors are stored as fixed-length arrays. For distance-based indexing a new index type, ANN, is implemented based on Faiss."}),"\n",(0,r.jsxs)(n.p,{children:["Using the common ",(0,r.jsx)(n.a,{href:"http://corpus-texmex.irisa.fr/",children:"SIFT"})," dataset as an example, you can create a table like this:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'CREATE TABLE sift_1M (\n  id int NOT NULL,\n  embedding array<float>  NOT NULL  COMMENT "",\n  INDEX ann_index (embedding) USING ANN PROPERTIES(\n      "index_type"="hnsw",\n      "metric_type"="l2_distance",\n      "dim"="128",\n      "quantizer"="flat"\n  )\n) ENGINE=OLAP\nDUPLICATE KEY(id) COMMENT "OLAP"\nDISTRIBUTED BY HASH(id) BUCKETS 1\nPROPERTIES (\n  "replication_num" = "1"\n);\n'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["index_type: ",(0,r.jsx)(n.code,{children:"hnsw"})," means using the ",(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Hierarchical_navigable_small_world",children:"Hierarchical Navigable Small World algorithm"})]}),"\n",(0,r.jsxs)(n.li,{children:["metric_type: ",(0,r.jsx)(n.code,{children:"l2_distance"})," means using L2 distance as the distance function"]}),"\n",(0,r.jsxs)(n.li,{children:["dim: ",(0,r.jsx)(n.code,{children:"128"})," means the vector dimension is 128"]}),"\n",(0,r.jsxs)(n.li,{children:["quantizer: ",(0,r.jsx)(n.code,{children:"flat"})," means each vector dimension is stored as original float32"]}),"\n"]}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Parameter"}),(0,r.jsx)(n.th,{children:"Required"}),(0,r.jsx)(n.th,{children:"Supported/Options"}),(0,r.jsx)(n.th,{children:"Default"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"index_type"})}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"hnsw only"}),(0,r.jsx)(n.td,{children:"(none)"}),(0,r.jsx)(n.td,{children:"ANN index algorithm. Currently only HNSW supported."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"metric_type"})}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"l2_distance"}),", ",(0,r.jsx)(n.code,{children:"inner_product"})]}),(0,r.jsx)(n.td,{children:"(none)"}),(0,r.jsx)(n.td,{children:"Vector similarity/distance metric. L2 = Euclidean; inner_product can approximate cosine if vectors are normalized."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"dim"})}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Positive integer (> 0)"}),(0,r.jsx)(n.td,{children:"(none)"}),(0,r.jsx)(n.td,{children:"Vector dimension. All imported vectors must match or an error is raised."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"max_degree"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Positive integer"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"32"})}),(0,r.jsx)(n.td,{children:"HNSW M (max neighbors per node). Affects index memory and search performance."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"ef_construction"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Positive integer"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"40"})}),(0,r.jsx)(n.td,{children:"HNSW efConstruction (candidate queue size during build). Larger gives better quality but slower build."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"quantizer"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"flat"}),", ",(0,r.jsx)(n.code,{children:"sq8"}),", ",(0,r.jsx)(n.code,{children:"sq4"})]}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"flat"})}),(0,r.jsxs)(n.td,{children:["Vector encoding/quantization: ",(0,r.jsx)(n.code,{children:"flat"})," = raw; ",(0,r.jsx)(n.code,{children:"sq8"}),"/",(0,r.jsx)(n.code,{children:"sq4"})," = symmetric quantization (8/4 bit) to reduce memory."]})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Import via S3 TVF:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:'INSERT INTO sift_1M\nSELECT *\nFROM S3("uri" =\n"https://selectdb-customers-tools-bj.oss-cn-beijing.aliyuncs.com/sift_database.tsv", "format" = "csv");\n\nselect count(*) from sift_1M\n\n+----------+\n| count(*) |\n+----------+\n|  1000000 |\n+----------+\n'})}),"\n",(0,r.jsx)(n.p,{children:"The SIFT dataset ships with a ground-truth set for result validation. Pick one query vector and first run an exact Top-N using the precise distance:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"SELECT id, l2_distance(embedding,     [0,11,77,24,3,0,0,0,28,70,125,8,0,0,0,0,44,35,50,45,9,0,0,0,4,0,4,56,18,0,3,9,16,17,59,10,10,8,57,57,100,105,125,41,1,0,6,92,8,14,73,125,29,7,0,5,0,0,8,124,66,6,3,1,63,5,0,1,49,32,17,35,125,21,0,3,2,12,6,109,21,0,0,35,74,125,14,23,0,0,6,50,25,70,64,7,59,18,7,16,22,5,0,1,125,23,1,0,7,30,14,32,4,0,2,2,59,125,19,4,0,0,2,1,6,53,33,2]) as distance FROM sift_1M ORDER BY distance limit 10\n--------------\n\n+--------+----------+\n| id     | distance |\n+--------+----------+\n| 178811 | 210.1595 |\n| 177646 | 217.0161 |\n| 181997 | 218.5406 |\n| 181605 | 219.2989 |\n| 821938 | 221.7228 |\n| 807785 | 226.7135 |\n| 716433 | 227.3148 |\n| 358802 | 230.7314 |\n| 803100 | 230.9112 |\n| 866737 | 231.6441 |\n+--------+----------+\n10 rows in set (0.29 sec)\n"})}),"\n",(0,r.jsxs)(n.p,{children:["When using ",(0,r.jsx)(n.code,{children:"l2_distance"})," or ",(0,r.jsx)(n.code,{children:"inner_product"}),", Doris computes the distance between the query vector and all 1,000,000 candidate vectors, then applies a TopN operator globally. Using ",(0,r.jsx)(n.code,{children:"l2_distance_approximate"})," / ",(0,r.jsx)(n.code,{children:"inner_product_approximate"})," triggers the index path:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"SELECT id, l2_distance_approximate(embedding,     [0,11,77,24,3,0,0,0,28,70,125,8,0,0,0,0,44,35,50,45,9,0,0,0,4,0,4,56,18,0,3,9,16,17,59,10,10,8,57,57,100,105,125,41,1,0,6,92,8,14,73,125,29,7,0,5,0,0,8,124,66,6,3,1,63,5,0,1,49,32,17,35,125,21,0,3,2,12,6,109,21,0,0,35,74,125,14,23,0,0,6,50,25,70,64,7,59,18,7,16,22,5,0,1,125,23,1,0,7,30,14,32,4,0,2,2,59,125,19,4,0,0,2,1,6,53,33,2]) as distance FROM sift_1M ORDER BY distance limit 10\n--------------\n\n+--------+----------+\n| id     | distance |\n+--------+----------+\n| 178811 | 210.1595 |\n| 177646 | 217.0161 |\n| 181997 | 218.5406 |\n| 181605 | 219.2989 |\n| 821938 | 221.7228 |\n| 807785 | 226.7135 |\n| 716433 | 227.3148 |\n| 358802 | 230.7314 |\n| 803100 | 230.9112 |\n| 866737 | 231.6441 |\n+--------+----------+\n10 rows in set (0.02 sec)\n"})}),"\n",(0,r.jsx)(n.p,{children:"With the ANN index, query latency in this example drops from about 290 ms to 20 ms."}),"\n",(0,r.jsx)(n.p,{children:"ANN indexes are built at the segment granularity. Because tables are distributed, after each segment returns its local TopN, the TopN operator merges results across tablets and segments to produce the global TopN."}),"\n",(0,r.jsxs)(n.p,{children:["Note: When ",(0,r.jsx)(n.code,{children:"metric_type = l2_distance"}),", a smaller distance means closer vectors. For ",(0,r.jsx)(n.code,{children:"inner_product"}),", a larger value means closer vectors. Therefore, if using ",(0,r.jsx)(n.code,{children:"inner_product"}),", you must use ",(0,r.jsx)(n.code,{children:"ORDER BY dist DESC"})," to obtain TopN via the index."]}),"\n",(0,r.jsx)(n.h2,{id:"approximate-range-search",children:"Approximate Range Search"}),"\n",(0,r.jsx)(n.p,{children:"Beyond the common TopN nearest neighbor search (returning the closest N records), another typical pattern is threshold-based range search. Instead of returning a fixed number of results, it returns all points whose distance to the target vector satisfies a predicate (>, >=, <, <=). For example, you might want vectors whose distance is greater than or less than a threshold. This is useful when you need candidates that are \u201Csufficiently similar\u201D or \u201Csufficiently dissimilar.\u201D In recommendation systems you might retrieve items that are close but not identical to improve diversity; in anomaly detection you look for points far from the normal distribution."}),"\n",(0,r.jsx)(n.p,{children:"Example SQL:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"SELECT  count(*) FROM sift_1M  WHERE l2_distance_approximate(embedding, [0,11,77,24,3,0,0,0,28,70,125,8,0,0,0,0,44,35,50,45,9,0,0,0,4,0,4,56,18,0,3,9,16,17,59,10,10,8,57,57,100,105,125,41,1,0,6,92,8,14,73,125,29,7,0,5,0,0,8,124,66,6,3,1,63,5,0,1,49,32,17,35,125,21,0,3,2,12,6,109,21,0,0,35,74,125,14,23,0,0,6,50,25,70,64,7,59,18,7,16,22,5,0,1,125,23,1,0,7,30,14,32,4,0,2,2,59,125,19,4,0,0,2,1,6,53,33,2]) > 300\n--------------\n\n+----------+\n| count(*) |\n+----------+\n|   999271 |\n+----------+\n1 row in set (0.19 sec)\n"})}),"\n",(0,r.jsxs)(n.p,{children:["These range-based vector searches are also accelerated by the ANN index: the index first narrows candidates, then approximate distances are computed, reducing cost and improving latency. Supported predicates: ",(0,r.jsx)(n.code,{children:">"}),", ",(0,r.jsx)(n.code,{children:">="}),", ",(0,r.jsx)(n.code,{children:"<"}),", ",(0,r.jsx)(n.code,{children:"<="}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"compound-search",children:"Compound Search"}),"\n",(0,r.jsx)(n.p,{children:"Compound Search combines an ANN TopN search with a range predicate in the same SQL statement, returning the TopN results that also satisfy a distance constraint."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"SELECT id, l2_distance_approximate(embedding, [0,11,77,24,3,0,0,0,28,70,125,8,0,0,0,0,44,35,50,45,9,0,0,0,4,0,4,56,18,0,3,9,16,17,59,10,10,8,57,57,100,105,125,41,1,0,6,92,8,14,73,125,29,7,0,5,0,0,8,124,66,6,3,1,63,5,0,1,49,32,17,35,125,21,0,3,2,12,6,109,21,0,0,35,74,125,14,23,0,0,6,50,25,70,64,7,59,18,7,16,22,5,0,1,125,23,1,0,7,30,14,32,4,0,2,2,59,125,19,4,0,0,2,1,6,53,33,2]) as dist FROM sift_1M  WHERE l2_distance_approximate(embedding, [0,11,77,24,3,0,0,0,28,70,125,8,0,0,0,0,44,35,50,45,9,0,0,0,4,0,4,56,18,0,3,9,16,17,59,10,10,8,57,57,100,105,125,41,1,0,6,92,8,14,73,125,29,7,0,5,0,0,8,124,66,6,3,1,63,5,0,1,49,32,17,35,125,21,0,3,2,12,6,109,21,0,0,35,74,125,14,23,0,0,6,50,25,70,64,7,59,18,7,16,22,5,0,1,125,23,1,0,7,30,14,32,4,0,2,2,59,125,19,4,0,0,2,1,6,53,33,2]) > 300 ORDER BY dist limit 10\n--------------\n\n+--------+----------+\n| id     | dist     |\n+--------+----------+\n| 243590 |  300.005 |\n| 549298 | 300.0317 |\n| 429685 | 300.0533 |\n| 690172 | 300.0916 |\n| 123410 | 300.1333 |\n| 232540 | 300.1649 |\n| 547696 | 300.2066 |\n| 855437 | 300.2782 |\n| 589017 | 300.3048 |\n| 930696 | 300.3381 |\n+--------+----------+\n10 rows in set (0.12 sec)\n"})}),"\n",(0,r.jsx)(n.p,{children:"A key question is whether predicate filtering happens before or after TopN. If predicates filter first and TopN is applied on the reduced set, it\u2019s pre-filtering; otherwise, it\u2019s post-filtering. Post-filtering can be faster but may dramatically reduce recall. Doris uses pre-filtering to preserve recall."}),"\n",(0,r.jsx)(n.p,{children:"Doris can accelerate both phases with the index. However, if the first phase (range filter) is too selective, indexing both phases can hurt recall. Doris adaptively decides whether to use the index twice based on predicate selectivity and index type."}),"\n",(0,r.jsx)(n.h2,{id:"ann-search-with-additional-filters",children:"ANN Search with Additional Filters"}),"\n",(0,r.jsx)(n.p,{children:"This refers to applying other predicates before the ANN TopN and returning the TopN under those constraints."}),"\n",(0,r.jsx)(n.p,{children:"Example with a small 8-D vector and a text filter:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'create table ann_with_fulltext (\n  id int not null,\n  embedding array<float> not null,\n  comment String not null,\n  value int null,\n  INDEX idx_comment(`comment`) USING INVERTED PROPERTIES("parser" = "english") COMMENT \'inverted index for comment\',\n  INDEX ann_embedding(`embedding`) USING ANN PROPERTIES("index_type"="hnsw","metric_type"="l2_distance","dim"="8")\n) duplicate key (`id`) \ndistributed by hash(`id`) buckets 1\nproperties("replication_num"="1");\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Insert sample data and search only within rows where ",(0,r.jsx)(n.code,{children:"comment"})," contains \u201Cmusic\u201D:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"INSERT INTO ann_with_fulltext VALUES\n(1, [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8], 'this is about music', 10),\n(2, [0.2,0.1,0.5,0.3,0.9,0.4,0.7,0.1], 'sports news today',   20),\n(3, [0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2], 'latest music trend',  30),\n(4, [0.05,0.06,0.07,0.08,0.09,0.1,0.2,0.3], 'politics update',40);\n\nSELECT id, comment,\n       l2_distance_approximate(embedding, [0.1,0.1,0.2,0.2,0.3,0.3,0.4,0.4]) AS dist\nFROM ann_with_fulltext\nWHERE comment MATCH_ANY 'music'       -- Filter using inverted index\nORDER BY dist ASC                     -- Ann topn calculation after predicates evaluate.\nLIMIT 2;\n\n+------+---------------------+----------+\n| id   | comment             | dist     |\n+------+---------------------+----------+\n|    1 | this is about music | 0.663325 |\n|    3 | latest music trend  | 1.280625 |\n+------+---------------------+----------+\n2 rows in set (0.04 sec)\n"})}),"\n",(0,r.jsx)(n.p,{children:"To ensure TopN can be accelerated via the vector index, all predicate columns should have appropriate secondary indexes (e.g., an inverted index)."}),"\n",(0,r.jsx)(n.h2,{id:"session-variables-related-to-ann-search",children:"Session Variables Related to ANN Search"}),"\n",(0,r.jsx)(n.p,{children:"Beyond build-time parameters for HNSW, you can pass search-time parameters via session variables:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"hnsw_ef_search: EF search parameter. Controls max length of the candidate queue; larger = higher accuracy, higher latency. Default 32."}),"\n",(0,r.jsx)(n.li,{children:"hnsw_check_relative_distance: Whether to enable relative distance checking to improve accuracy. Default true."}),"\n",(0,r.jsx)(n.li,{children:"hnsw_bounded_queue: Whether to use a bounded priority queue to optimize performance. Default true."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"vector-quantization",children:"Vector Quantization"}),"\n",(0,r.jsx)(n.p,{children:"With FLAT encoding, an HNSW index (raw vectors plus graph structure) may consume large amounts of memory. HNSW must be fully resident in memory to function, so memory can become a bottleneck at large scale."}),"\n",(0,r.jsx)(n.p,{children:"Vector quantization compresses float32 storage to reduce memory. Doris currently supports two scalar quantization schemes: INT8 and INT4 (SQ8 / SQ4). Example using SQ8:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'CREATE TABLE sift_1M (\n  id int NOT NULL,\n  embedding array<float>  NOT NULL  COMMENT "",\n  INDEX ann_index (embedding) USING ANN PROPERTIES(\n      "index_type"="hnsw",\n      "metric_type"="l2_distance",\n      "dim"="128",\n      "quantizer"="sq8"\n  )\n) ENGINE=OLAP\nDUPLICATE KEY(id) COMMENT "OLAP"\nDISTRIBUTED BY HASH(id) BUCKETS 1\nPROPERTIES (\n  "replication_num" = "1"\n);\n'})}),"\n",(0,r.jsx)(n.p,{children:"On 768-D Cohere-MEDIUM-1M and Cohere-LARGE-10M datasets, SQ8 reduces index size to roughly one third compared to FLAT."}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Dataset"}),(0,r.jsx)(n.th,{children:"Dim"}),(0,r.jsx)(n.th,{children:"Storage/Index Scheme"}),(0,r.jsx)(n.th,{children:"Total Disk"}),(0,r.jsx)(n.th,{children:"Data Part"}),(0,r.jsx)(n.th,{children:"Index Part"}),(0,r.jsx)(n.th,{children:"Notes"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Cohere-MEDIUM-1M"}),(0,r.jsx)(n.td,{children:"768D"}),(0,r.jsx)(n.td,{children:"Doris (FLAT)"}),(0,r.jsx)(n.td,{children:"5.647 GB (2.533 + 3.114)"}),(0,r.jsx)(n.td,{children:"2.533 GB"}),(0,r.jsx)(n.td,{children:"3.114 GB"}),(0,r.jsx)(n.td,{children:"1M vectors"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Cohere-MEDIUM-1M"}),(0,r.jsx)(n.td,{children:"768D"}),(0,r.jsx)(n.td,{children:"Doris SQ INT8"}),(0,r.jsx)(n.td,{children:"3.501 GB (2.533 + 0.992)"}),(0,r.jsx)(n.td,{children:"2.533 GB"}),(0,r.jsx)(n.td,{children:"0.992 GB"}),(0,r.jsx)(n.td,{children:"INT8 symmetric quantization"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Cohere-LARGE-10M"}),(0,r.jsx)(n.td,{children:"768D"}),(0,r.jsx)(n.td,{children:"Doris (FLAT)"}),(0,r.jsx)(n.td,{children:"56.472 GB (25.328 + 31.145)"}),(0,r.jsx)(n.td,{children:"25.328 GB"}),(0,r.jsx)(n.td,{children:"31.145 GB"}),(0,r.jsx)(n.td,{children:"10M vectors"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Cohere-LARGE-10M"}),(0,r.jsx)(n.td,{children:"768D"}),(0,r.jsx)(n.td,{children:"Doris SQ INT8"}),(0,r.jsx)(n.td,{children:"35.016 GB (25.329 + 9.687)"}),(0,r.jsx)(n.td,{children:"25.329 GB"}),(0,r.jsx)(n.td,{children:"9.687 GB"}),(0,r.jsx)(n.td,{children:"INT8 quantization"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Quantization introduces extra build-time overhead because each distance computation must decode quantized values. For 128-D vectors, build time increases with row count; SQ vs. FLAT can be up to ~10\xd7 slower to build."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"ANN-SQ-BUILD_COSTS",src:i(571545).Z+"",width:"1231",height:"728"})}),"\n",(0,r.jsx)(n.h2,{id:"performance-tuning",children:"Performance Tuning"}),"\n",(0,r.jsx)(n.p,{children:"Vector search is a typical secondary-index point lookup scenario. For high QPS and low latency, consider the following:"}),"\n",(0,r.jsx)(n.p,{children:"With tuning, on hardware FE 32C 64GB + BE 32C 64GB, Doris can reach 3000+ QPS (dataset: Cohere-MEDIUM-1M)."}),"\n",(0,r.jsx)(n.h3,{id:"query-performance",children:"Query Performance"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Concurrency"}),(0,r.jsx)(n.th,{children:"Scheme"}),(0,r.jsx)(n.th,{children:"QPS"}),(0,r.jsx)(n.th,{children:"Avg Latency (s)"}),(0,r.jsx)(n.th,{children:"P99 (s)"}),(0,r.jsx)(n.th,{children:"CPU Usage"}),(0,r.jsx)(n.th,{children:"Recall"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"240"}),(0,r.jsx)(n.td,{children:"Doris"}),(0,r.jsx)(n.td,{children:"3340.4399"}),(0,r.jsx)(n.td,{children:"0.071368168"}),(0,r.jsx)(n.td,{children:"0.163399825"}),(0,r.jsx)(n.td,{children:"40%"}),(0,r.jsx)(n.td,{children:"91.00%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"240"}),(0,r.jsx)(n.td,{children:"Doris SQ INT8"}),(0,r.jsx)(n.td,{children:"3188.6359"}),(0,r.jsx)(n.td,{children:"0.074728852"}),(0,r.jsx)(n.td,{children:"0.160370195"}),(0,r.jsx)(n.td,{children:"40%"}),(0,r.jsx)(n.td,{children:"88.26%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"240"}),(0,r.jsx)(n.td,{children:"Doris SQ INT4"}),(0,r.jsx)(n.td,{children:"2818.2291"}),(0,r.jsx)(n.td,{children:"0.084663868"}),(0,r.jsx)(n.td,{children:"0.174826815"}),(0,r.jsx)(n.td,{children:"43%"}),(0,r.jsx)(n.td,{children:"80.38%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"240"}),(0,r.jsx)(n.td,{children:"Doris brute force"}),(0,r.jsx)(n.td,{children:"3.6787"}),(0,r.jsx)(n.td,{children:"25.554878826"}),(0,r.jsx)(n.td,{children:"29.363227973"}),(0,r.jsx)(n.td,{children:"100%"}),(0,r.jsx)(n.td,{children:"100.00%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"480"}),(0,r.jsx)(n.td,{children:"Doris"}),(0,r.jsx)(n.td,{children:"4155.7220"}),(0,r.jsx)(n.td,{children:"0.113387271"}),(0,r.jsx)(n.td,{children:"0.261086075"}),(0,r.jsx)(n.td,{children:"60%"}),(0,r.jsx)(n.td,{children:"91.00%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"480"}),(0,r.jsx)(n.td,{children:"Doris SQ INT8"}),(0,r.jsx)(n.td,{children:"3833.1130"}),(0,r.jsx)(n.td,{children:"0.123040214"}),(0,r.jsx)(n.td,{children:"0.276912867"}),(0,r.jsx)(n.td,{children:"50%"}),(0,r.jsx)(n.td,{children:"88.26%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"480"}),(0,r.jsx)(n.td,{children:"Doris SQ INT4"}),(0,r.jsx)(n.td,{children:"3431.0538"}),(0,r.jsx)(n.td,{children:"0.137636995"}),(0,r.jsx)(n.td,{children:"0.281631249"}),(0,r.jsx)(n.td,{children:"57%"}),(0,r.jsx)(n.td,{children:"80.38%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"480"}),(0,r.jsx)(n.td,{children:"Doris brute force"}),(0,r.jsx)(n.td,{children:"3.6787"}),(0,r.jsx)(n.td,{children:"25.554878826"}),(0,r.jsx)(n.td,{children:"29.363227973"}),(0,r.jsx)(n.td,{children:"100%"}),(0,r.jsx)(n.td,{children:"100.00%"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"use-prepared-statements",children:"Use Prepared Statements"}),"\n",(0,r.jsx)(n.p,{children:"Modern embedding models often output 768-D or higher vectors. If you inline a 768-D literal into SQL, parsing time can exceed execution time. Use prepared statements. Currently Doris does not support MySQL client prepare commands directly; use JDBC:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Enable server-side prepared statements in the JDBC URL:",(0,r.jsx)(n.br,{}),"\n",(0,r.jsx)(n.code,{children:"jdbc:mysql://127.0.0.1:9030/demo?useServerPrepStmts=true"})]}),"\n",(0,r.jsxs)(n.li,{children:["Use PreparedStatement with placeholders (",(0,r.jsx)(n.code,{children:"?"}),") and reuse it."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"reduce-segment-count",children:"Reduce Segment Count"}),"\n",(0,r.jsxs)(n.p,{children:["ANN indexes are built per segment. Too many segments cause overhead. Ideally each tablet should have no more than ~5 segments for an ANN-indexed table. Adjust ",(0,r.jsx)(n.code,{children:"write_buffer_size"})," and ",(0,r.jsx)(n.code,{children:"vertical_compaction_max_segment_size"})," in ",(0,r.jsx)(n.code,{children:"be.conf"})," (e.g., both to 10737418240)."]}),"\n",(0,r.jsx)(n.h3,{id:"reduce-rowset-count",children:"Reduce Rowset Count"}),"\n",(0,r.jsxs)(n.p,{children:["Same motivation as reducing segments: minimize scheduling overhead. Each load creates a rowset, so prefer stream load or ",(0,r.jsx)(n.code,{children:"INSERT INTO SELECT"})," for batched ingestion."]}),"\n",(0,r.jsx)(n.h3,{id:"keep-ann-index-in-memory",children:"Keep ANN Index in Memory"}),"\n",(0,r.jsxs)(n.p,{children:["Current ANN algorithms are memory-based. If a segment\u2019s index is not in memory, a disk I/O occurs. Set ",(0,r.jsx)(n.code,{children:"enable_segment_cache_prune=false"})," in ",(0,r.jsx)(n.code,{children:"be.conf"})," to keep ANN indexes resident."]}),"\n",(0,r.jsx)(n.h3,{id:"parallel_pipeline_task_num--1",children:"parallel_pipeline_task_num = 1"}),"\n",(0,r.jsxs)(n.p,{children:["ANN TopN queries return very few rows from each scanner, so high pipeline task parallelism is unnecessary. Set ",(0,r.jsx)(n.code,{children:"parallel_pipeline_task_num = 1"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"enable_profile--false",children:"enable_profile = false"}),"\n",(0,r.jsx)(n.p,{children:"Disable query profiling for ultra latency-sensitive queries."}),"\n",(0,r.jsx)(n.h2,{id:"usage-limitations",children:"Usage Limitations"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["The ANN index column must be a NOT NULL ",(0,r.jsx)(n.code,{children:"Array<Float>"}),", and every imported vector must match the declared ",(0,r.jsx)(n.code,{children:"dim"}),", otherwise an error is thrown."]}),"\n",(0,r.jsx)(n.li,{children:"ANN index is only supported on DuplicateKey table model."}),"\n",(0,r.jsxs)(n.li,{children:["Doris uses pre-filter semantics (predicates applied before ANN TopN). If predicates include columns without secondary indexes that can precisely locate rows (e.g., no inverted index), Doris falls back to brute force to preserve correctness.\nExample:\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"SELECT id, l2_distance_approximate(embedding, [xxx]) AS distance\nFROM sift_1M\nWHERE round(id) > 100\nORDER BY distance LIMIT 10;\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Although ",(0,r.jsx)(n.code,{children:"id"})," is a key, without a secondary index (such as an inverted index), its predicate is applied after index analysis, so Doris falls back to brute force to honor pre-filter semantics.\n4. If the distance function in SQL does not match the metric type defined in the index DDL, Doris cannot use the ANN index for TopN\u2014even if you call ",(0,r.jsx)(n.code,{children:"l2_distance_approximate"})," / ",(0,r.jsx)(n.code,{children:"inner_product_approximate"}),".\n5. For metric type ",(0,r.jsx)(n.code,{children:"inner_product"}),", only ",(0,r.jsx)(n.code,{children:"ORDER BY inner_product_approximate(...) DESC LIMIT N"})," (DESC required) can be accelerated by the ANN index.\n6. The first parameter of ",(0,r.jsx)(n.code,{children:"xxx_approximate()"})," must be a ColumnArray, and the second must be a CAST or ArrayLiteral. Reversing them triggers brute-force search."]})]})}function h(e={}){let{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(o,{...e})}):o(e)}},571545:function(e,n,i){i.d(n,{Z:function(){return t}});let t=i.p+"assets/images/ann-sq-build-time-21e62d015837dbe7238fecc474515e27.png"},250065:function(e,n,i){i.d(n,{Z:function(){return a},a:function(){return d}});var t=i(667294);let r={},s=t.createContext(r);function d(e){let n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:d(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);